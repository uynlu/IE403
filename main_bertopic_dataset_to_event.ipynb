{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908120f6",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31043520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rreip\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.backend import BaseEmbedder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import itertools\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from underthesea import word_tokenize\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257ca53",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d257e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_content(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    title = \"\"\n",
    "    content_lines = []\n",
    "    in_content = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Title:\"):\n",
    "            title = line.replace(\"Title:\", \"\", 1).strip()\n",
    "        elif line.startswith(\"Content:\"):\n",
    "            in_content = True\n",
    "            continue  # b·ªè d√≤ng \"Content:\"\n",
    "        elif in_content:\n",
    "            content_lines.append(line.rstrip())\n",
    "\n",
    "    content = \". \" + \"\\n\".join(content_lines)\n",
    "    result = (title + \"\\n\" + content).replace(\"\\n\", \" \")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b817b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C√°c n∆∞·ªõc chia bu·ªìn v·ª• tai n·∫°n m√°y bay c·ªßa Ai C·∫≠p . T·ªïng th∆∞ k√Ω T·ªï ch·ª©c Hi·ªáp ∆∞·ªõc B·∫Øc ƒê·∫°i T√¢y D∆∞∆°ng (NATO) Jens Stoltenberg ng√†y 19/5 cho bi·∫øt, n·∫øu Ai C·∫≠p ƒë·ªÅ ngh·ªã, li√™n minh n√†y s·∫Ω h·ªó tr·ª£ c√¥ng t√°c t√¨m ki·∫øm chi·∫øc m√°y bay mang s·ªë hi·ªáu MS 804 c·ªßa h√£ng h√†ng kh√¥ng Ai C·∫≠p ch·ªü 66 ng∆∞·ªùi m·∫•t t√≠ch tr∆∞·ªõc ƒë√≥ c√πng ng√†y. ‚ÄúT√¥i g·ª≠i l·ªùi chia bu·ªìn s√¢u s·∫Øc nh·∫•t ƒë·∫øn nh·ªØng ai b·ªã ·∫£nh h∆∞·ªüng b·ªüi v·ª• vi·ªác n√†y. T√¥i c≈©ng g·ª≠i l·ªùi chia bu·ªìn s√¢u s·∫Øc ƒë·∫øn Ph√°p v√† Ai C·∫≠p. T√¥i bi·∫øt r·∫±ng ƒë√£ c√≥ nh·ªØng n·ªó l·ª±c t√¨m ki·∫øm c·ª©u n·∫°n ·ªü m·ª©c ƒë·ªô qu·ªëc gia. Ph√°p v√† Ai C·∫≠p ƒëang ph·ªëi h·ª£p trong c√¥ng t√°c n√†y c≈©ng nh∆∞ vi·ªác ƒëi·ªÅu tra. Ch√∫ng t√¥i s·∫Ω ti·∫øp t·ª•c theo d√µi ch·∫∑t ch·∫Ω di·ªÖn bi·∫øn v√† n·∫øu ƒë∆∞·ª£c ƒë·ªÅ ngh·ªã, NATO lu√¥n s·∫µn s√†ng gi√∫p ƒë·ª°‚Äù, √¥ng Jens Stoltenberg n√≥i. Th·ªß t∆∞·ªõng Italy Matteo Renzi ng√†y 19/5 c≈©ng ƒë√£ g·ª≠i l·ªùi chia bu·ªìn, ƒë·ªìng th·ªùi b√†y t·ªè s·ª± ƒëo√†n k·∫øt v·ªõi Ai C·∫≠p sau v·ª• m√°y bay c·ªßa h√£ng h√†ng kh√¥ng Ai C·∫≠p m·∫•t t√≠ch tr√™n ƒê·ªãa Trung H·∫£i khi ƒëang tr√™n ƒë∆∞·ªùng bay t·ª´ Paris ƒë·∫øn Cairo. Tr∆∞·ªõc ƒë√≥, H√£ng h√†ng kh√¥ng qu·ªëc gia Ai C·∫≠p (EgyptAir) x√°c nh·∫≠n ph√≠a Hy L·∫°p ƒë√£ t√¨m th·∫•y m·∫£nh v·ª° t·ª´ chi·∫øc m√°y bay n√†y ·ªü ph√≠a Nam ƒë·∫£o Karpathos, thu·ªôc v√πng Nam ƒê·ªãa Trung H·∫£i. H√£ng ƒë√£ g·ª≠i l·ªùi chia bu·ªìn ƒë·∫øn gia ƒë√¨nh c√°c h√†nh kh√°ch tr√™n chuy·∫øn bay m·∫•t t√≠ch nh∆∞ m·ªôt s·ª± x√°c nh·∫≠n ƒë·∫ßu ti√™n r·∫±ng th√¢n nh√¢n c·ªßa h·ªç ƒë√£ qua ƒë·ªùi. H√£ng c≈©ng cam k·∫øt s·∫Ω tri·ªÉn khai m·ªçi bi·ªán ph√°p gi·∫£i quy·∫øt t√¨nh h√¨nh hi·ªán nay c≈©ng nh∆∞ ti·∫øn h√†nh m·ªôt cu·ªôc ƒëi·ªÅu tra t·ªïng th·ªÉ. Ng∆∞·ªùi ƒë·ª©ng ƒë·∫ßu c∆° quan ƒëi·ªÅu tra tai n·∫°n h√†ng kh√¥ng Ai C·∫≠p Ayman al-Moqadem ng√†y 19/5 cho bi·∫øt, n∆∞·ªõc n√†y s·∫Ω d·∫´n ƒë·∫ßu m·ªôt ·ªßy ban ƒëi·ªÅu tra v·ªÅ v·ª• m·∫•t t√≠ch chi·∫øc m√°y bay mang s·ªë hi·ªáu MS 804. ·ª¶y ban n√†y bao g·ªìm c·∫£ nh√¢n s·ª± ph√≠a Ph√°p, n∆∞·ªõc s·∫£n xu·∫•t chi·∫øc Airbus 320 n√†y v√† c≈©ng l√† n∆∞·ªõc c√≥ s·ªë n·∫°n nh√¢n nhi·ªÅu th·ª© hai sau Ai C·∫≠p. C∆° quan ch·ª©c nƒÉng Ph√°p ƒë√£ kh·∫≥ng ƒë·ªãnh s·∫Ω c·ª≠ 3 chuy√™n gia sang Ai C·∫≠p tham gia ƒëi·ªÅu tra v·ª• tai n·∫°n m√°y bay n√†y. Anh v√† Hy L·∫°p c≈©ng ƒë√£ ƒë·ªÅ ngh·ªã gi√∫p ƒë·ª° t√¨m ki·∫øm h·ªôp ƒëen v√† nh·ªØng m·∫£nh v·ª° c·ªßa chi·∫øc m√°y bay. H·ªôi ƒë·ªìng an to√†n giao th√¥ng qu·ªëc gia M·ªπ cho bi·∫øt, ƒë·ªông c∆° c·ªßa chi·∫øc m√°y bay g·∫∑p n·∫°n ƒë∆∞·ª£c s·∫£n xu·∫•t t·∫°i n∆∞·ªõc n√†y. Theo quy t·∫Øc qu·ªëc t·∫ø, n∆∞·ªõc n∆°i ƒë·ªông c∆° m√°y bay ƒë∆∞·ª£c ch·∫ø t·∫°o c≈©ng c√≥ th·ªÉ tham gia v√†o cu·ªôc ƒëi·ªÅu tra khi tai n·∫°n x·∫£y ra. Hi·ªán M·ªπ ƒë√£ c·ª≠ m√°y bay P-3 Orion h·ªó tr·ª£ c√¥ng t√°c t√¨m ki·∫øm chi·∫øc m√°y bay m·∫•t t√≠ch c·ªßa Ai C·∫≠p. L√∫c n√†y, ·ª©ng vi√™n T·ªïng th·ªëng ƒë·∫£ng C·ªông h√≤a M·ªπ Donald Trump ƒë√£ l√™n ti·∫øng b√†y t·ªè nghi ng·ªù ƒë√¢y l√† m·ªôt v·ª• t·∫•n c√¥ng kh·ªßng b·ªë song ch√≠nh ph·ªß M·ªπ cho r·∫±ng, v·ª• tai n·∫°n m√°y bay v·∫´n ƒëang ƒë∆∞·ª£c ƒëi·ªÅu tra v√† c√≤n qu√° s·ªõm ƒë·ªÉ x√°c ƒë·ªãnh nguy√™n nh√¢n khi·∫øn m√°y bay g·∫∑p n·∫°n. Th·ªß t∆∞·ªõng Ai C·∫≠p Sherif Ismail th√¨ nh·∫≠n ƒë·ªãnh c√≤n qu√° s·ªõm ƒë·ªÅ lo·∫°i b·ªè b·∫•t c·ª© gi·∫£ thuy·∫øt n√†o, k·ªÉ c·∫£ tr∆∞·ªùng h·ª£p m√°y bay b·ªã kh·ªßng b·ªë. B·ªô tr∆∞·ªüng B·ªô H√†ng kh√¥ng d√¢n d·ª•ng Ai C·∫≠p Sherif Phathi c≈©ng cho r·∫±ng, kh·∫£ nƒÉng m√°y bay b·ªã kh·ªßng b·ªë cao h∆°n kh·∫£ nƒÉng x·∫£y ra l·ªói k·ªπ thu·∫≠t d√π √¥ng ch∆∞a ƒë∆∞a ra b·∫±ng ch·ª©ng c·ª• th·ªÉ n√†o. T·ªïng th·ªëng Ai C·∫≠p Mohamed Morsi ƒë√£ y√™u c·∫ßu B·ªô H√†ng kh√¥ng d√¢n d·ª•ng v√† qu√¢n ƒë·ªôi ph·ªëi h·ª£p nhanh ch√≥ng ƒë·ªãnh v·ªã n∆°i chi·∫øc m√°y bay mang s·ªë hi·ªáu MS 804 r∆°i v√† ti·∫øn h√†nh m·ªôt cu·ªôc ƒëi·ªÅu tra th·∫•u ƒë√°o. Trong khi ƒë√≥, Ngo·∫°i tr∆∞·ªüng Canada Stephane Dion ng√†y 19/5 cho bi·∫øt trong s·ªë nh·ªØng h√†nh kh√°ch ƒëi chuy·∫øn bay mang s·ªë hi·ªáu MS 804 c·ªßa H√£ng h√†ng kh√¥ng qu·ªëc gia Ai C·∫≠p (EgyptAir) b·ªã m·∫•t t√≠ch c√πng ng√†y c√≥ √≠t nh·∫•t 2 c√¥ng d√¢n n∆∞·ªõc n√†y. √îng c≈©ng cho bi·∫øt B·ªô Ngo·∫°i giao Canada ƒëang ph·ªëi h·ª£p v·ªõi c√°c ƒë·ªëi t√°c Ph√°p v√† Ai C·∫≠p, c≈©ng nh∆∞ c√°c n∆∞·ªõc li√™n quan kh√°c ƒë·ªÉ ƒë√°nh gi√° t√¨nh h√¨nh v√† xem x√©t c√°c y√™u c·∫ßu h·ªó tr·ª£. Tr∆∞·ªõc ƒë√≥, h√£ng h√†ng kh√¥ng qu·ªëc gia Ai C·∫≠p ƒë√£ c√¥ng b·ªë qu·ªëc t·ªãch c·ªßa nh·ªØng h√†nh kh√°ch ƒëi tr√™n chuy·∫øn bay MS 804 b·ªã m·∫•t t√≠ch, bao g·ªìm 56 h√†nh kh√°ch, trong ƒë√≥ c√≥ 30 ng∆∞·ªùi Ai C·∫≠p, 15 ng∆∞·ªùi Ph√°p, 2 ng∆∞·ªùi I-r·∫Øc, 1 ng∆∞·ªùi Anh, 1 ng∆∞·ªùi B·ªâ, 1 ng∆∞·ªùi Kuwait, 1 ng∆∞·ªùi Saudi Arabia, 1 ng∆∞·ªùi Sudan, 1 ng∆∞·ªùi C·ªông h√≤a Chad, 1 ng∆∞·ªùi B·ªì ƒê√†o Nha, 1 ng∆∞·ªùi Angeria v√† 1 ng∆∞·ªùi Canada. Ngo√†i ra, c√≤n c√≥ 10 th√†nh vi√™n phi h√†nh ƒëo√†n./.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\before_bertopic_dataset\\Cluster_001\\original\\1.txt\"\n",
    "output_path = extract_title_and_content(input_path)\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a79f289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\before_bertopic_dataset\"\n",
    "all_clusters_docs = []\n",
    "\n",
    "for i in range(1, 301):\n",
    "    cluster_name = f\"Cluster_{i:03d}\"\n",
    "    original_path = os.path.join(base_dir, cluster_name, \"original\")\n",
    "\n",
    "    cluster_docs = []\n",
    "\n",
    "    if os.path.exists(original_path):\n",
    "        for filename in os.listdir(original_path):\n",
    "            file_path = os.path.join(original_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                cleaned_text = extract_title_and_content(file_path)\n",
    "                all_clusters_docs.append(cleaned_text)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {original_path}\")\n",
    "\n",
    "# Ghi ra file t·ªïng h·ª£p\n",
    "output_path = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\all_events_dataset\\all_events_dataset_khong_theo_event.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_clusters_docs, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3e95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\all_events_dataset\\all_events_dataset_khong_theo_event.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_of_docs = json.load(f)\n",
    "\n",
    "print(len(list_of_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417e675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  event_id                                               text\n",
      "0        0         1  C√°c n∆∞·ªõc chia bu·ªìn v·ª• tai n·∫°n m√°y bay c·ªßa Ai C...\n",
      "1        1         1  M√°y bay Ai C·∫≠p r∆°i: Nh·ªØng c√¢u h·ªèi cho ch√≠nh ph...\n",
      "2        2         1  Gi·∫£i m√£ b√≠ ·∫©n m√°y bay r∆°i c·ªßa EgyptAir . Ph√≥ C...\n",
      "3        3         1  Ph√°t hi·ªán m·∫£nh v·ª° nghi c·ªßa m√°y bay MS804 g·∫∑p n...\n",
      "4        4         1  M√°y bay EgyptAir c√≥ th·ªÉ b·ªã t·∫•n c√¥ng b·∫±ng t√™n l...\n",
      "...    ...       ...                                                ...\n",
      "1940  1940       300  X·ª≠ l√Ω xong s·ª± c·ªë ·ªü s√¢n bay Bu√¥n Ma Thu·ªôt . Chi...\n",
      "1941  1941       300  ƒê√£ kh·∫Øc ph·ª•c xong s·ª± c·ªë ƒë∆∞·ªùng bƒÉng s√¢n bay Bu√¥...\n",
      "1942  1942       300  Kh·∫Øc ph·ª•c xong s·ª± c·ªë C·∫£ng h√†ng kh√¥ng Bu√¥n Ma T...\n",
      "1943  1943       300  Kh·∫Øc ph·ª•c xong s·ª± c·ªë t·∫°i C·∫£ng H√†ng kh√¥ng Bu√¥n ...\n",
      "1944  1944       300  S√¢n bay Bu√¥n M√™ Thu·ªôt ho·∫°t ƒë·ªông tr·ªü l·∫°i . Tr∆∞·ªõ...\n",
      "\n",
      "[1945 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_dataframe(data):\n",
    "    rows = []\n",
    "    for event_id, articles in enumerate(data):\n",
    "        for article in articles:\n",
    "            rows.append({\n",
    "                \"event_id\": event_id + 1,\n",
    "                \"text\": article\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    return df[[\"id\", \"event_id\", \"text\"]]\n",
    "\n",
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\all_events_dataset\\all_events_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "df = convert_to_dataframe(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817016e",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0b512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePairDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=256):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, text2, label = self.pairs[idx]\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text1,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs2 = self.tokenizer(\n",
    "            text2,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids_1': inputs1['input_ids'].squeeze(0),\n",
    "            'attention_mask_1': inputs1['attention_mask'].squeeze(0),\n",
    "            'input_ids_2': inputs2['input_ids'].squeeze(0),\n",
    "            'attention_mask_2': inputs2['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0ec995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contrastive_pairs(df, max_neg_per_pos=1):\n",
    "    from collections import defaultdict\n",
    "    import random\n",
    "\n",
    "    grouped = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        grouped[row['event_id']].append(row['text'])\n",
    "\n",
    "    pairs = []\n",
    "    event_ids = list(grouped.keys())\n",
    "    for eid in event_ids:\n",
    "        texts = grouped[eid]\n",
    "        for i in range(len(texts)):\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                pairs.append((texts[i], texts[j], 1.0))  # positive pair\n",
    "\n",
    "                for _ in range(max_neg_per_pos):\n",
    "                    neg_eid = random.choice([e for e in event_ids if e != eid])\n",
    "                    neg_text = random.choice(grouped[neg_eid])\n",
    "                    pairs.append((texts[i], neg_text, 0.0))  # negative pair\n",
    "\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(model, input_ids, attention_mask):\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return output.last_hidden_state[:, 0]  # CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a285d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_phobert_contrastive(model, tokenizer, pairs, device=\"cpu\", epochs=1, batch_size=8, lr=2e-5):\n",
    "    dataset = ContrastivePairDataset(pairs, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            input_ids_1 = batch['input_ids_1'].to(device)\n",
    "            attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "            input_ids_2 = batch['input_ids_2'].to(device)\n",
    "            attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "            labels = batch['label'].to(device) * 2 - 1  # convert 0/1 to -1/+1\n",
    "\n",
    "            emb1 = get_cls_embedding(model, input_ids_1, attention_mask_1)\n",
    "            emb2 = get_cls_embedding(model, input_ids_2, attention_mask_2)\n",
    "\n",
    "            loss = loss_fn(emb1, emb2, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d261cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1434/1434 [3:41:09<00:00,  9.25s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 145.6047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('phobert_event_embedding\\\\tokenizer_config.json',\n",
       " 'phobert_event_embedding\\\\special_tokens_map.json',\n",
       " 'phobert_event_embedding\\\\vocab.txt',\n",
       " 'phobert_event_embedding\\\\bpe.codes',\n",
       " 'phobert_event_embedding\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kh·ªüi t·∫°o model/tokenizer\n",
    "model_name = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Chu·∫©n b·ªã data\n",
    "pairs = make_contrastive_pairs(df)\n",
    "\n",
    "# Hu·∫•n luy·ªán\n",
    "trained_model = train_phobert_contrastive(model, tokenizer, pairs)\n",
    "trained_model.save_pretrained(\"phobert_event_embedding\")\n",
    "tokenizer.save_pretrained(\"phobert_event_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15626def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBERTEmbedder:\n",
    "    def __init__(self, model_name=\"vinai/phobert-base\", batch_size=32, max_length=256): \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, documents, show_progress_bar=False):\n",
    "        self.model.eval()\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(documents), self.batch_size)\n",
    "        if show_progress_bar:\n",
    "            iterator = tqdm(iterator, desc=\"Embedding documents\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_docs = documents[i:i+self.batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch_docs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**tokens)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            all_embeddings.append(cls_embeddings.cpu())\n",
    "\n",
    "        return torch.cat(all_embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877ca62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61/61 [06:09<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "phobert_embedder = PhoBERTEmbedder(model_name=\"phobert_event_embedding\")\n",
    "embeddings = phobert_embedder(df['text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc032dd",
   "metadata": {},
   "source": [
    "# UMAP + HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid tham s·ªë\n",
    "umap_n_neighbors = [4, 7, 10]\n",
    "umap_n_components = [5]\n",
    "umap_min_dist = [0.0, 0.1] \n",
    "umap_metric = [\"cosine\"] #, \"manhattan\", \"euclidean\"\n",
    "\n",
    "hdbscan_min_cluster_size = [4, 6, 8]\n",
    "hdbscan_cluster_selection_method = ['eom', 'leaf'] \n",
    "hdbscan_metric = ['euclidean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a485df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªï h·ª£p tham s·ªë\n",
    "param_grid = list(itertools.product(\n",
    "    umap_n_neighbors,\n",
    "    umap_n_components,\n",
    "    umap_min_dist,\n",
    "    umap_metric,\n",
    "    hdbscan_min_cluster_size,\n",
    "    hdbscan_cluster_selection_method,\n",
    "    hdbscan_metric\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c00ee9",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7375e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªçc stopword list t·ª´ file\n",
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    my_stopwords = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc7c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=my_stopwords, ngram_range=(1, 2)) #max_df=0.9, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb52a2",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keybert_model = KeyBERTInspired()\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b2644",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e229640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Grid 1/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.641 | Outlier Ratio: 3.29%\n",
      "\n",
      "üîç Grid 2/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.641 | Outlier Ratio: 3.29%\n",
      "\n",
      "üîç Grid 3/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.424 | Outlier Ratio: 14.86%\n",
      "\n",
      "üîç Grid 4/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.434 | Outlier Ratio: 14.96%\n",
      "\n",
      "üîç Grid 5/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.395 | Outlier Ratio: 19.43%\n",
      "\n",
      "üîç Grid 6/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.362 | Outlier Ratio: 21.59%\n",
      "\n",
      "üîç Grid 7/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.584 | Outlier Ratio: 3.91%\n",
      "\n",
      "üîç Grid 8/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.564 | Outlier Ratio: 4.63%\n",
      "\n",
      "üîç Grid 9/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.358 | Outlier Ratio: 15.42%\n",
      "\n",
      "üîç Grid 10/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.377 | Outlier Ratio: 18.05%\n",
      "\n",
      "üîç Grid 11/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.311 | Outlier Ratio: 21.54%\n",
      "\n",
      "üîç Grid 12/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.31 | Outlier Ratio: 25.09%\n",
      "\n",
      "üîç Grid 13/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.581 | Outlier Ratio: 5.24%\n",
      "\n",
      "üîç Grid 14/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.57 | Outlier Ratio: 5.66%\n",
      "\n",
      "üîç Grid 15/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.434 | Outlier Ratio: 14.09%\n",
      "\n",
      "üîç Grid 16/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.421 | Outlier Ratio: 15.94%\n",
      "\n",
      "üîç Grid 17/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.341 | Outlier Ratio: 20.15%\n",
      "\n",
      "üîç Grid 18/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.352 | Outlier Ratio: 23.44%\n",
      "\n",
      "üîç Grid 19/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.543 | Outlier Ratio: 7.15%\n",
      "\n",
      "üîç Grid 20/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.539 | Outlier Ratio: 7.51%\n",
      "\n",
      "üîç Grid 21/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.367 | Outlier Ratio: 16.81%\n",
      "\n",
      "üîç Grid 22/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.367 | Outlier Ratio: 19.33%\n",
      "\n",
      "üîç Grid 23/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.287 | Outlier Ratio: 26.17%\n",
      "\n",
      "üîç Grid 24/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.288 | Outlier Ratio: 28.84%\n",
      "\n",
      "üîç Grid 25/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.537 | Outlier Ratio: 6.74%\n",
      "\n",
      "üîç Grid 26/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.525 | Outlier Ratio: 7.61%\n",
      "\n",
      "üîç Grid 27/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.391 | Outlier Ratio: 17.63%\n",
      "\n",
      "üîç Grid 28/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.385 | Outlier Ratio: 18.25%\n",
      "\n",
      "üîç Grid 29/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.3 | Outlier Ratio: 25.24%\n",
      "\n",
      "üîç Grid 30/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.303 | Outlier Ratio: 26.89%\n",
      "\n",
      "üîç Grid 31/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.47 | Outlier Ratio: 9.72%\n",
      "\n",
      "üîç Grid 32/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.461 | Outlier Ratio: 9.92%\n",
      "\n",
      "üîç Grid 33/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.379 | Outlier Ratio: 22.57%\n",
      "\n",
      "üîç Grid 34/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.363 | Outlier Ratio: 24.94%\n",
      "\n",
      "üîç Grid 35/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "üìä DBCV: 0.261 | Outlier Ratio: 28.28%\n",
      "\n",
      "üîç Grid 36/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "üìä DBCV: 0.278 | Outlier Ratio: 33.62%\n",
      "(4, 5, 0.0, 'cosine', 4, 'eom', 'euclidean')\n"
     ]
    }
   ],
   "source": [
    "# T√¨m best_model\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for i, (n_neighbors, n_components, min_dist, umap_metric, min_cluster_size, cluster_selection_method, hdbscan_metric) in enumerate(param_grid):\n",
    "    print(f\"\\nüîç Grid {i+1}/{len(param_grid)}: UMAP(n_neighbors={n_neighbors}, min_dist={min_dist}, metric={umap_metric}), \"\n",
    "        f\"HDBSCAN(min_cluster_size={min_cluster_size}, cluster_selection_method={cluster_selection_method}, metric={hdbscan_metric})\")\n",
    "\n",
    "    # T·∫°o UMAP v√† HDBSCAN\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=min_dist, metric=umap_metric, random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, cluster_selection_method=cluster_selection_method, metric=hdbscan_metric, prediction_data=True, gen_min_span_tree=True)\n",
    "\n",
    "    # BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        # embedding_model=lambda docs: phobert_embedder(docs, show_progress_bar=False),\n",
    "        embedding_model=lambda docs: embeddings(docs, show_progress_bar=False),\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        # ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        language=\"multilingual\",\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    topics, _ = topic_model.fit_transform(df['text'].tolist())\n",
    "    # topics, _ = topic_model.fit_transform(list_of_docs, embeddings)\n",
    "\n",
    "    # T√≠nh DBCV score\n",
    "    dbcv_score = topic_model.hdbscan_model.relative_validity_\n",
    "\n",
    "    # T√≠nh % outliers\n",
    "    outlier_ratio = topics.count(-1) / len(topics)\n",
    "\n",
    "    print(f\"üìä DBCV: {round(dbcv_score, 3)} | Outlier Ratio: {round(outlier_ratio * 100, 2)}%\")\n",
    "\n",
    "    # K·∫øt h·ª£p: ∆Øu ti√™n DBCV cao nh·∫•t, sau ƒë√≥ l√† outlier th·∫•p\n",
    "    composite_score = dbcv_score - outlier_ratio  # ho·∫∑c d√πng tr·ªçng s·ªë: alpha*DBCV - beta*outlier\n",
    "\n",
    "    if composite_score > best_score:\n",
    "        best_score = composite_score\n",
    "        best_model = topic_model\n",
    "        best_params = (n_neighbors, n_components, min_dist, umap_metric, min_cluster_size, cluster_selection_method, hdbscan_metric)\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "topics, probs = best_model.fit_transform(df['text'].tolist())\n",
    "# topics, probs = best_model.fit_transform(list_of_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c99bdcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DBCV: 0.641 | Outlier Ratio: 3.29%\n"
     ]
    }
   ],
   "source": [
    "# T√≠nh DBCV score\n",
    "dbcv_score = best_model.hdbscan_model.relative_validity_\n",
    "\n",
    "# T√≠nh % outliers\n",
    "outlier_ratio = topics.count(-1) / len(topics)\n",
    "\n",
    "print(f\"üìä DBCV: {round(dbcv_score, 3)} | Outlier Ratio: {round(outlier_ratio * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373cd998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C√°c n∆∞·ªõc chia bu·ªìn v·ª• tai n·∫°n m√°y bay c·ªßa Ai C...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M√°y bay Ai C·∫≠p r∆°i: Nh·ªØng c√¢u h·ªèi cho ch√≠nh ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Gi·∫£i m√£ b√≠ ·∫©n m√°y bay r∆°i c·ªßa EgyptAir . Ph√≥ C...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Ph√°t hi·ªán m·∫£nh v·ª° nghi c·ªßa m√°y bay MS804 g·∫∑p n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>M√°y bay EgyptAir c√≥ th·ªÉ b·ªã t·∫•n c√¥ng b·∫±ng t√™n l...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>1940</td>\n",
       "      <td>300</td>\n",
       "      <td>X·ª≠ l√Ω xong s·ª± c·ªë ·ªü s√¢n bay Bu√¥n Ma Thu·ªôt . Chi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>1941</td>\n",
       "      <td>300</td>\n",
       "      <td>ƒê√£ kh·∫Øc ph·ª•c xong s·ª± c·ªë ƒë∆∞·ªùng bƒÉng s√¢n bay Bu√¥...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>1942</td>\n",
       "      <td>300</td>\n",
       "      <td>Kh·∫Øc ph·ª•c xong s·ª± c·ªë C·∫£ng h√†ng kh√¥ng Bu√¥n Ma T...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>1943</td>\n",
       "      <td>300</td>\n",
       "      <td>Kh·∫Øc ph·ª•c xong s·ª± c·ªë t·∫°i C·∫£ng H√†ng kh√¥ng Bu√¥n ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>1944</td>\n",
       "      <td>300</td>\n",
       "      <td>S√¢n bay Bu√¥n M√™ Thu·ªôt ho·∫°t ƒë·ªông tr·ªü l·∫°i . Tr∆∞·ªõ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  event_id                                               text  topic\n",
       "0        0         1  C√°c n∆∞·ªõc chia bu·ªìn v·ª• tai n·∫°n m√°y bay c·ªßa Ai C...     23\n",
       "1        1         1  M√°y bay Ai C·∫≠p r∆°i: Nh·ªØng c√¢u h·ªèi cho ch√≠nh ph...      0\n",
       "2        2         1  Gi·∫£i m√£ b√≠ ·∫©n m√°y bay r∆°i c·ªßa EgyptAir . Ph√≥ C...     74\n",
       "3        3         1  Ph√°t hi·ªán m·∫£nh v·ª° nghi c·ªßa m√°y bay MS804 g·∫∑p n...     -1\n",
       "4        4         1  M√°y bay EgyptAir c√≥ th·ªÉ b·ªã t·∫•n c√¥ng b·∫±ng t√™n l...     42\n",
       "...    ...       ...                                                ...    ...\n",
       "1940  1940       300  X·ª≠ l√Ω xong s·ª± c·ªë ·ªü s√¢n bay Bu√¥n Ma Thu·ªôt . Chi...      6\n",
       "1941  1941       300  ƒê√£ kh·∫Øc ph·ª•c xong s·ª± c·ªë ƒë∆∞·ªùng bƒÉng s√¢n bay Bu√¥...      6\n",
       "1942  1942       300  Kh·∫Øc ph·ª•c xong s·ª± c·ªë C·∫£ng h√†ng kh√¥ng Bu√¥n Ma T...      6\n",
       "1943  1943       300  Kh·∫Øc ph·ª•c xong s·ª± c·ªë t·∫°i C·∫£ng H√†ng kh√¥ng Bu√¥n ...      6\n",
       "1944  1944       300  S√¢n bay Bu√¥n M√™ Thu·ªôt ho·∫°t ƒë·ªông tr·ªü l·∫°i . Tr∆∞·ªõ...      6\n",
       "\n",
       "[1945 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'] = topics\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f85100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.7968\n",
      "NMI: 0.9597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_eval = df[df[\"topic\"] != -1].copy()\n",
    "ari = adjusted_rand_score(df_eval[\"event_id\"], df_eval[\"topic\"])\n",
    "nmi = normalized_mutual_info_score(df_eval[\"event_id\"], df_eval[\"topic\"])\n",
    "\n",
    "print(f\"ARI: {ari:.4f}\")\n",
    "print(f\"NMI: {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4068228",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_docs = defaultdict(list)\n",
    "\n",
    "for doc, topic in zip(list_of_docs, topics):\n",
    "    topic_to_docs[topic].append(doc)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"Topic\": topic, \"Docs\": topic_docs, \"Count\": len(topic_docs)}\n",
    "    for topic, topic_docs in topic_to_docs.items()\n",
    "])\n",
    "\n",
    "result = []\n",
    "for _, row in df.iterrows():\n",
    "    result.append({\n",
    "        \"topic\": row[\"Topic\"],\n",
    "        \"count\": row[\"Count\"],\n",
    "        \"docs\": row[\"Docs\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24429cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open(fr\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\ƒê·ªì √°n\\Event_Cluster.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
