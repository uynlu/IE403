{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908120f6",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31043520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rreip\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.backend import BaseEmbedder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import itertools\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from underthesea import word_tokenize\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257ca53",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d257e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_content(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    title = \"\"\n",
    "    content_lines = []\n",
    "    in_content = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Title:\"):\n",
    "            title = line.replace(\"Title:\", \"\", 1).strip()\n",
    "        elif line.startswith(\"Content:\"):\n",
    "            in_content = True\n",
    "            continue  # bỏ dòng \"Content:\"\n",
    "        elif in_content:\n",
    "            content_lines.append(line.rstrip())\n",
    "\n",
    "    content = \". \" + \"\\n\".join(content_lines)\n",
    "    result = (title + \"\\n\" + content).replace(\"\\n\", \" \")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b817b414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Các nước chia buồn vụ tai nạn máy bay của Ai Cập . Tổng thư ký Tổ chức Hiệp ước Bắc Đại Tây Dương (NATO) Jens Stoltenberg ngày 19/5 cho biết, nếu Ai Cập đề nghị, liên minh này sẽ hỗ trợ công tác tìm kiếm chiếc máy bay mang số hiệu MS 804 của hãng hàng không Ai Cập chở 66 người mất tích trước đó cùng ngày. “Tôi gửi lời chia buồn sâu sắc nhất đến những ai bị ảnh hưởng bởi vụ việc này. Tôi cũng gửi lời chia buồn sâu sắc đến Pháp và Ai Cập. Tôi biết rằng đã có những nỗ lực tìm kiếm cứu nạn ở mức độ quốc gia. Pháp và Ai Cập đang phối hợp trong công tác này cũng như việc điều tra. Chúng tôi sẽ tiếp tục theo dõi chặt chẽ diễn biến và nếu được đề nghị, NATO luôn sẵn sàng giúp đỡ”, ông Jens Stoltenberg nói. Thủ tướng Italy Matteo Renzi ngày 19/5 cũng đã gửi lời chia buồn, đồng thời bày tỏ sự đoàn kết với Ai Cập sau vụ máy bay của hãng hàng không Ai Cập mất tích trên Địa Trung Hải khi đang trên đường bay từ Paris đến Cairo. Trước đó, Hãng hàng không quốc gia Ai Cập (EgyptAir) xác nhận phía Hy Lạp đã tìm thấy mảnh vỡ từ chiếc máy bay này ở phía Nam đảo Karpathos, thuộc vùng Nam Địa Trung Hải. Hãng đã gửi lời chia buồn đến gia đình các hành khách trên chuyến bay mất tích như một sự xác nhận đầu tiên rằng thân nhân của họ đã qua đời. Hãng cũng cam kết sẽ triển khai mọi biện pháp giải quyết tình hình hiện nay cũng như tiến hành một cuộc điều tra tổng thể. Người đứng đầu cơ quan điều tra tai nạn hàng không Ai Cập Ayman al-Moqadem ngày 19/5 cho biết, nước này sẽ dẫn đầu một ủy ban điều tra về vụ mất tích chiếc máy bay mang số hiệu MS 804. Ủy ban này bao gồm cả nhân sự phía Pháp, nước sản xuất chiếc Airbus 320 này và cũng là nước có số nạn nhân nhiều thứ hai sau Ai Cập. Cơ quan chức năng Pháp đã khẳng định sẽ cử 3 chuyên gia sang Ai Cập tham gia điều tra vụ tai nạn máy bay này. Anh và Hy Lạp cũng đã đề nghị giúp đỡ tìm kiếm hộp đen và những mảnh vỡ của chiếc máy bay. Hội đồng an toàn giao thông quốc gia Mỹ cho biết, động cơ của chiếc máy bay gặp nạn được sản xuất tại nước này. Theo quy tắc quốc tế, nước nơi động cơ máy bay được chế tạo cũng có thể tham gia vào cuộc điều tra khi tai nạn xảy ra. Hiện Mỹ đã cử máy bay P-3 Orion hỗ trợ công tác tìm kiếm chiếc máy bay mất tích của Ai Cập. Lúc này, ứng viên Tổng thống đảng Cộng hòa Mỹ Donald Trump đã lên tiếng bày tỏ nghi ngờ đây là một vụ tấn công khủng bố song chính phủ Mỹ cho rằng, vụ tai nạn máy bay vẫn đang được điều tra và còn quá sớm để xác định nguyên nhân khiến máy bay gặp nạn. Thủ tướng Ai Cập Sherif Ismail thì nhận định còn quá sớm đề loại bỏ bất cứ giả thuyết nào, kể cả trường hợp máy bay bị khủng bố. Bộ trưởng Bộ Hàng không dân dụng Ai Cập Sherif Phathi cũng cho rằng, khả năng máy bay bị khủng bố cao hơn khả năng xảy ra lỗi kỹ thuật dù ông chưa đưa ra bằng chứng cụ thể nào. Tổng thống Ai Cập Mohamed Morsi đã yêu cầu Bộ Hàng không dân dụng và quân đội phối hợp nhanh chóng định vị nơi chiếc máy bay mang số hiệu MS 804 rơi và tiến hành một cuộc điều tra thấu đáo. Trong khi đó, Ngoại trưởng Canada Stephane Dion ngày 19/5 cho biết trong số những hành khách đi chuyến bay mang số hiệu MS 804 của Hãng hàng không quốc gia Ai Cập (EgyptAir) bị mất tích cùng ngày có ít nhất 2 công dân nước này. Ông cũng cho biết Bộ Ngoại giao Canada đang phối hợp với các đối tác Pháp và Ai Cập, cũng như các nước liên quan khác để đánh giá tình hình và xem xét các yêu cầu hỗ trợ. Trước đó, hãng hàng không quốc gia Ai Cập đã công bố quốc tịch của những hành khách đi trên chuyến bay MS 804 bị mất tích, bao gồm 56 hành khách, trong đó có 30 người Ai Cập, 15 người Pháp, 2 người I-rắc, 1 người Anh, 1 người Bỉ, 1 người Kuwait, 1 người Saudi Arabia, 1 người Sudan, 1 người Cộng hòa Chad, 1 người Bồ Đào Nha, 1 người Angeria và 1 người Canada. Ngoài ra, còn có 10 thành viên phi hành đoàn./.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\before_bertopic_dataset\\Cluster_001\\original\\1.txt\"\n",
    "output_path = extract_title_and_content(input_path)\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a79f289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\before_bertopic_dataset\"\n",
    "all_clusters_docs = []\n",
    "\n",
    "for i in range(1, 301):\n",
    "    cluster_name = f\"Cluster_{i:03d}\"\n",
    "    original_path = os.path.join(base_dir, cluster_name, \"original\")\n",
    "\n",
    "    cluster_docs = []\n",
    "\n",
    "    if os.path.exists(original_path):\n",
    "        for filename in os.listdir(original_path):\n",
    "            file_path = os.path.join(original_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                cleaned_text = extract_title_and_content(file_path)\n",
    "                all_clusters_docs.append(cleaned_text)\n",
    "    else:\n",
    "        print(f\"⚠️ Không tìm thấy thư mục: {original_path}\")\n",
    "\n",
    "# Ghi ra file tổng hợp\n",
    "output_path = r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\all_events_dataset\\all_events_dataset_khong_theo_event.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_clusters_docs, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3e95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\all_events_dataset\\all_events_dataset_khong_theo_event.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_of_docs = json.load(f)\n",
    "\n",
    "print(len(list_of_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417e675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  event_id                                               text\n",
      "0        0         1  Các nước chia buồn vụ tai nạn máy bay của Ai C...\n",
      "1        1         1  Máy bay Ai Cập rơi: Những câu hỏi cho chính ph...\n",
      "2        2         1  Giải mã bí ẩn máy bay rơi của EgyptAir . Phó C...\n",
      "3        3         1  Phát hiện mảnh vỡ nghi của máy bay MS804 gặp n...\n",
      "4        4         1  Máy bay EgyptAir có thể bị tấn công bằng tên l...\n",
      "...    ...       ...                                                ...\n",
      "1940  1940       300  Xử lý xong sự cố ở sân bay Buôn Ma Thuột . Chi...\n",
      "1941  1941       300  Đã khắc phục xong sự cố đường băng sân bay Buô...\n",
      "1942  1942       300  Khắc phục xong sự cố Cảng hàng không Buôn Ma T...\n",
      "1943  1943       300  Khắc phục xong sự cố tại Cảng Hàng không Buôn ...\n",
      "1944  1944       300  Sân bay Buôn Mê Thuột hoạt động trở lại . Trướ...\n",
      "\n",
      "[1945 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_dataframe(data):\n",
    "    rows = []\n",
    "    for event_id, articles in enumerate(data):\n",
    "        for article in articles:\n",
    "            rows.append({\n",
    "                \"event_id\": event_id + 1,\n",
    "                \"text\": article\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    return df[[\"id\", \"event_id\", \"text\"]]\n",
    "\n",
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\all_events_dataset\\all_events_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "df = convert_to_dataframe(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817016e",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0b512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePairDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length=256):\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, text2, label = self.pairs[idx]\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text1,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs2 = self.tokenizer(\n",
    "            text2,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids_1': inputs1['input_ids'].squeeze(0),\n",
    "            'attention_mask_1': inputs1['attention_mask'].squeeze(0),\n",
    "            'input_ids_2': inputs2['input_ids'].squeeze(0),\n",
    "            'attention_mask_2': inputs2['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0ec995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contrastive_pairs(df, max_neg_per_pos=1):\n",
    "    from collections import defaultdict\n",
    "    import random\n",
    "\n",
    "    grouped = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        grouped[row['event_id']].append(row['text'])\n",
    "\n",
    "    pairs = []\n",
    "    event_ids = list(grouped.keys())\n",
    "    for eid in event_ids:\n",
    "        texts = grouped[eid]\n",
    "        for i in range(len(texts)):\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                pairs.append((texts[i], texts[j], 1.0))  # positive pair\n",
    "\n",
    "                for _ in range(max_neg_per_pos):\n",
    "                    neg_eid = random.choice([e for e in event_ids if e != eid])\n",
    "                    neg_text = random.choice(grouped[neg_eid])\n",
    "                    pairs.append((texts[i], neg_text, 0.0))  # negative pair\n",
    "\n",
    "    random.shuffle(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(model, input_ids, attention_mask):\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return output.last_hidden_state[:, 0]  # CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a285d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_phobert_contrastive(model, tokenizer, pairs, device=\"cpu\", epochs=1, batch_size=8, lr=2e-5):\n",
    "    dataset = ContrastivePairDataset(pairs, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            input_ids_1 = batch['input_ids_1'].to(device)\n",
    "            attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "            input_ids_2 = batch['input_ids_2'].to(device)\n",
    "            attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "            labels = batch['label'].to(device) * 2 - 1  # convert 0/1 to -1/+1\n",
    "\n",
    "            emb1 = get_cls_embedding(model, input_ids_1, attention_mask_1)\n",
    "            emb2 = get_cls_embedding(model, input_ids_2, attention_mask_2)\n",
    "\n",
    "            loss = loss_fn(emb1, emb2, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d261cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1434/1434 [3:41:09<00:00,  9.25s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 145.6047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('phobert_event_embedding\\\\tokenizer_config.json',\n",
       " 'phobert_event_embedding\\\\special_tokens_map.json',\n",
       " 'phobert_event_embedding\\\\vocab.txt',\n",
       " 'phobert_event_embedding\\\\bpe.codes',\n",
       " 'phobert_event_embedding\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Khởi tạo model/tokenizer\n",
    "model_name = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Chuẩn bị data\n",
    "pairs = make_contrastive_pairs(df)\n",
    "\n",
    "# Huấn luyện\n",
    "trained_model = train_phobert_contrastive(model, tokenizer, pairs)\n",
    "trained_model.save_pretrained(\"phobert_event_embedding\")\n",
    "tokenizer.save_pretrained(\"phobert_event_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15626def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBERTEmbedder:\n",
    "    def __init__(self, model_name=\"vinai/phobert-base\", batch_size=32, max_length=256): \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, documents, show_progress_bar=False):\n",
    "        self.model.eval()\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(documents), self.batch_size)\n",
    "        if show_progress_bar:\n",
    "            iterator = tqdm(iterator, desc=\"Embedding documents\")\n",
    "\n",
    "        for i in iterator:\n",
    "            batch_docs = documents[i:i+self.batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch_docs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**tokens)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            all_embeddings.append(cls_embeddings.cpu())\n",
    "\n",
    "        return torch.cat(all_embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877ca62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding documents: 100%|██████████| 61/61 [06:09<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "phobert_embedder = PhoBERTEmbedder(model_name=\"phobert_event_embedding\")\n",
    "embeddings = phobert_embedder(df['text'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc032dd",
   "metadata": {},
   "source": [
    "# UMAP + HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid tham số\n",
    "umap_n_neighbors = [4, 7, 10]\n",
    "umap_n_components = [5]\n",
    "umap_min_dist = [0.0, 0.1] \n",
    "umap_metric = [\"cosine\"] #, \"manhattan\", \"euclidean\"\n",
    "\n",
    "hdbscan_min_cluster_size = [4, 6, 8]\n",
    "hdbscan_cluster_selection_method = ['eom', 'leaf'] \n",
    "hdbscan_metric = ['euclidean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a485df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tổ hợp tham số\n",
    "param_grid = list(itertools.product(\n",
    "    umap_n_neighbors,\n",
    "    umap_n_components,\n",
    "    umap_min_dist,\n",
    "    umap_metric,\n",
    "    hdbscan_min_cluster_size,\n",
    "    hdbscan_cluster_selection_method,\n",
    "    hdbscan_metric\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c00ee9",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7375e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc stopword list từ file\n",
    "with open(r\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    my_stopwords = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc7c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=my_stopwords, ngram_range=(1, 2)) #max_df=0.9, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb52a2",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keybert_model = KeyBERTInspired()\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b2644",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e229640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Grid 1/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.641 | Outlier Ratio: 3.29%\n",
      "\n",
      "🔍 Grid 2/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.641 | Outlier Ratio: 3.29%\n",
      "\n",
      "🔍 Grid 3/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.424 | Outlier Ratio: 14.86%\n",
      "\n",
      "🔍 Grid 4/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.434 | Outlier Ratio: 14.96%\n",
      "\n",
      "🔍 Grid 5/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.395 | Outlier Ratio: 19.43%\n",
      "\n",
      "🔍 Grid 6/36: UMAP(n_neighbors=4, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.362 | Outlier Ratio: 21.59%\n",
      "\n",
      "🔍 Grid 7/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.584 | Outlier Ratio: 3.91%\n",
      "\n",
      "🔍 Grid 8/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.564 | Outlier Ratio: 4.63%\n",
      "\n",
      "🔍 Grid 9/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.358 | Outlier Ratio: 15.42%\n",
      "\n",
      "🔍 Grid 10/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.377 | Outlier Ratio: 18.05%\n",
      "\n",
      "🔍 Grid 11/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.311 | Outlier Ratio: 21.54%\n",
      "\n",
      "🔍 Grid 12/36: UMAP(n_neighbors=4, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.31 | Outlier Ratio: 25.09%\n",
      "\n",
      "🔍 Grid 13/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.581 | Outlier Ratio: 5.24%\n",
      "\n",
      "🔍 Grid 14/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.57 | Outlier Ratio: 5.66%\n",
      "\n",
      "🔍 Grid 15/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.434 | Outlier Ratio: 14.09%\n",
      "\n",
      "🔍 Grid 16/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.421 | Outlier Ratio: 15.94%\n",
      "\n",
      "🔍 Grid 17/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.341 | Outlier Ratio: 20.15%\n",
      "\n",
      "🔍 Grid 18/36: UMAP(n_neighbors=7, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.352 | Outlier Ratio: 23.44%\n",
      "\n",
      "🔍 Grid 19/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.543 | Outlier Ratio: 7.15%\n",
      "\n",
      "🔍 Grid 20/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.539 | Outlier Ratio: 7.51%\n",
      "\n",
      "🔍 Grid 21/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.367 | Outlier Ratio: 16.81%\n",
      "\n",
      "🔍 Grid 22/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.367 | Outlier Ratio: 19.33%\n",
      "\n",
      "🔍 Grid 23/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.287 | Outlier Ratio: 26.17%\n",
      "\n",
      "🔍 Grid 24/36: UMAP(n_neighbors=7, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.288 | Outlier Ratio: 28.84%\n",
      "\n",
      "🔍 Grid 25/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.537 | Outlier Ratio: 6.74%\n",
      "\n",
      "🔍 Grid 26/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.525 | Outlier Ratio: 7.61%\n",
      "\n",
      "🔍 Grid 27/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.391 | Outlier Ratio: 17.63%\n",
      "\n",
      "🔍 Grid 28/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.385 | Outlier Ratio: 18.25%\n",
      "\n",
      "🔍 Grid 29/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.3 | Outlier Ratio: 25.24%\n",
      "\n",
      "🔍 Grid 30/36: UMAP(n_neighbors=10, min_dist=0.0, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.303 | Outlier Ratio: 26.89%\n",
      "\n",
      "🔍 Grid 31/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.47 | Outlier Ratio: 9.72%\n",
      "\n",
      "🔍 Grid 32/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=4, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.461 | Outlier Ratio: 9.92%\n",
      "\n",
      "🔍 Grid 33/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.379 | Outlier Ratio: 22.57%\n",
      "\n",
      "🔍 Grid 34/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=6, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.363 | Outlier Ratio: 24.94%\n",
      "\n",
      "🔍 Grid 35/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=eom, metric=euclidean)\n",
      "📊 DBCV: 0.261 | Outlier Ratio: 28.28%\n",
      "\n",
      "🔍 Grid 36/36: UMAP(n_neighbors=10, min_dist=0.1, metric=cosine), HDBSCAN(min_cluster_size=8, cluster_selection_method=leaf, metric=euclidean)\n",
      "📊 DBCV: 0.278 | Outlier Ratio: 33.62%\n",
      "(4, 5, 0.0, 'cosine', 4, 'eom', 'euclidean')\n"
     ]
    }
   ],
   "source": [
    "# Tìm best_model\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for i, (n_neighbors, n_components, min_dist, umap_metric, min_cluster_size, cluster_selection_method, hdbscan_metric) in enumerate(param_grid):\n",
    "    print(f\"\\n🔍 Grid {i+1}/{len(param_grid)}: UMAP(n_neighbors={n_neighbors}, min_dist={min_dist}, metric={umap_metric}), \"\n",
    "        f\"HDBSCAN(min_cluster_size={min_cluster_size}, cluster_selection_method={cluster_selection_method}, metric={hdbscan_metric})\")\n",
    "\n",
    "    # Tạo UMAP và HDBSCAN\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=min_dist, metric=umap_metric, random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, cluster_selection_method=cluster_selection_method, metric=hdbscan_metric, prediction_data=True, gen_min_span_tree=True)\n",
    "\n",
    "    # BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        # embedding_model=lambda docs: phobert_embedder(docs, show_progress_bar=False),\n",
    "        embedding_model=lambda docs: embeddings(docs, show_progress_bar=False),\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        # ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        language=\"multilingual\",\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    topics, _ = topic_model.fit_transform(df['text'].tolist())\n",
    "    # topics, _ = topic_model.fit_transform(list_of_docs, embeddings)\n",
    "\n",
    "    # Tính DBCV score\n",
    "    dbcv_score = topic_model.hdbscan_model.relative_validity_\n",
    "\n",
    "    # Tính % outliers\n",
    "    outlier_ratio = topics.count(-1) / len(topics)\n",
    "\n",
    "    print(f\"📊 DBCV: {round(dbcv_score, 3)} | Outlier Ratio: {round(outlier_ratio * 100, 2)}%\")\n",
    "\n",
    "    # Kết hợp: Ưu tiên DBCV cao nhất, sau đó là outlier thấp\n",
    "    composite_score = dbcv_score - outlier_ratio  # hoặc dùng trọng số: alpha*DBCV - beta*outlier\n",
    "\n",
    "    if composite_score > best_score:\n",
    "        best_score = composite_score\n",
    "        best_model = topic_model\n",
    "        best_params = (n_neighbors, n_components, min_dist, umap_metric, min_cluster_size, cluster_selection_method, hdbscan_metric)\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "topics, probs = best_model.fit_transform(df['text'].tolist())\n",
    "# topics, probs = best_model.fit_transform(list_of_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c99bdcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DBCV: 0.641 | Outlier Ratio: 3.29%\n"
     ]
    }
   ],
   "source": [
    "# Tính DBCV score\n",
    "dbcv_score = best_model.hdbscan_model.relative_validity_\n",
    "\n",
    "# Tính % outliers\n",
    "outlier_ratio = topics.count(-1) / len(topics)\n",
    "\n",
    "print(f\"📊 DBCV: {round(dbcv_score, 3)} | Outlier Ratio: {round(outlier_ratio * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373cd998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Các nước chia buồn vụ tai nạn máy bay của Ai C...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Máy bay Ai Cập rơi: Những câu hỏi cho chính ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Giải mã bí ẩn máy bay rơi của EgyptAir . Phó C...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Phát hiện mảnh vỡ nghi của máy bay MS804 gặp n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Máy bay EgyptAir có thể bị tấn công bằng tên l...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>1940</td>\n",
       "      <td>300</td>\n",
       "      <td>Xử lý xong sự cố ở sân bay Buôn Ma Thuột . Chi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>1941</td>\n",
       "      <td>300</td>\n",
       "      <td>Đã khắc phục xong sự cố đường băng sân bay Buô...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>1942</td>\n",
       "      <td>300</td>\n",
       "      <td>Khắc phục xong sự cố Cảng hàng không Buôn Ma T...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>1943</td>\n",
       "      <td>300</td>\n",
       "      <td>Khắc phục xong sự cố tại Cảng Hàng không Buôn ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>1944</td>\n",
       "      <td>300</td>\n",
       "      <td>Sân bay Buôn Mê Thuột hoạt động trở lại . Trướ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  event_id                                               text  topic\n",
       "0        0         1  Các nước chia buồn vụ tai nạn máy bay của Ai C...     23\n",
       "1        1         1  Máy bay Ai Cập rơi: Những câu hỏi cho chính ph...      0\n",
       "2        2         1  Giải mã bí ẩn máy bay rơi của EgyptAir . Phó C...     74\n",
       "3        3         1  Phát hiện mảnh vỡ nghi của máy bay MS804 gặp n...     -1\n",
       "4        4         1  Máy bay EgyptAir có thể bị tấn công bằng tên l...     42\n",
       "...    ...       ...                                                ...    ...\n",
       "1940  1940       300  Xử lý xong sự cố ở sân bay Buôn Ma Thuột . Chi...      6\n",
       "1941  1941       300  Đã khắc phục xong sự cố đường băng sân bay Buô...      6\n",
       "1942  1942       300  Khắc phục xong sự cố Cảng hàng không Buôn Ma T...      6\n",
       "1943  1943       300  Khắc phục xong sự cố tại Cảng Hàng không Buôn ...      6\n",
       "1944  1944       300  Sân bay Buôn Mê Thuột hoạt động trở lại . Trướ...      6\n",
       "\n",
       "[1945 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'] = topics\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f85100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.7968\n",
      "NMI: 0.9597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_eval = df[df[\"topic\"] != -1].copy()\n",
    "ari = adjusted_rand_score(df_eval[\"event_id\"], df_eval[\"topic\"])\n",
    "nmi = normalized_mutual_info_score(df_eval[\"event_id\"], df_eval[\"topic\"])\n",
    "\n",
    "print(f\"ARI: {ari:.4f}\")\n",
    "print(f\"NMI: {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4068228",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_docs = defaultdict(list)\n",
    "\n",
    "for doc, topic in zip(list_of_docs, topics):\n",
    "    topic_to_docs[topic].append(doc)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"Topic\": topic, \"Docs\": topic_docs, \"Count\": len(topic_docs)}\n",
    "    for topic, topic_docs in topic_to_docs.items()\n",
    "])\n",
    "\n",
    "result = []\n",
    "for _, row in df.iterrows():\n",
    "    result.append({\n",
    "        \"topic\": row[\"Topic\"],\n",
    "        \"count\": row[\"Count\"],\n",
    "        \"docs\": row[\"Docs\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24429cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open(fr\"C:\\Users\\rreip\\Downloads\\HK6\\IE403\\Đồ án\\Event_Cluster.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
